---
title: 'Lecture 12: Sampling Biased'
author: "Catie McVey"
date: "2023-11-7"
output: html_document
---

```{r}
library(tidyverse)
```


A couple weeks ago we talked about missing values. They are the bane of every statisticians existance, because they show up in almost every real world dataset. In that lecture we discussed the difference between missing values that are randomly distributed within a dataset vs those that systematically generated by some underlying mecahnism. We talked about how nonrandom missing values can bias our analyses, just like in the case of the dead cows. Systematic missing values are a special case of "sampling bias". Today we are going to talk about that issue a bit more generally. 

Sampling bias occurs any time that your sampling stratedgy generates a sample of data that does not mirror the demographics of the reference population from which it is drawn. If some subsector of a population is more likely to generate missing values, and those incomplete records are just tossed out, then the sample will no longer mirror the demographics of the reference population. But you don't have to have missing values to bias a sample. In fact, missing values can be a boon in an analysis, because they provide clear visual evidence in your dataset that something is amiss. Sampling bias can be harder to spot, however, when you have non-response

![Nonresponse](Nonresponse.jpg)
Now the term nonresponse clearly comes from survey sampling world, but free choice doesn't necessarily have to be an element of the sampling design for bias to creep in. For example, sampling bias is also seen quite commonly in seen in wildlife studies. If they want to study the rodent population in an area, they'll bury little pit traps into the ground that critters fall in as they move around at night. A squirrel doesn't exactly opt-in/out of such a study. But you can imagine that certain animals may be more likely to fall into such a trap if they are more active than other parts of the population - you could easily catch nothing but males during certain parts fo the breeding season, or tend to catch younger animals with less defined home bases, or there are even studies that such samples are biased towards animals with more "bold/active" personality traits. If the thing that you are studying is not identical across these subpopulations, then you could get a very misleading analysis. 

So how do you pick this up. The short answer is - its hard. You often need to collect a lot of metadata from your sample, and compare it to known demographics of your reference population to make sure that everything matches up. This takes a lot of fore-thought, and often quite a bit of domain expertise. And there will be plenty of scenarios where either that metadata wasn't collected in the first place (usually when a scientist designs a study without consulting a statistician first) or there are no reliable demographics available for the reference population. In such scenarios, there are limits to what you can do analytically to re-balance your data and guard against biased inferences. Crap in, crap out. In this situation, the most important role in this analysis is to make sure your client/audience understand the limitations of the dataset itself, and thus potential skew of any subsequent inferences. I would argue that including a clear visualization of any metadata avaiable for the sample is something you should not only create, but encourage your client to include in any subsequent communications, so that any downstream reader can quickly and comprehensively explore the shortcomings in the dataset and apply their own domain expertise about the impacts of those sampling challenges in interperating any statistical results that your report. 

# Dealing with Sampling Bias - Weighted Regression

Alright, so lets suppose you are in a scenario where either your client comes to your aware of sampling bias in their dataset, or else you've identified it through your Exploratory Data Analysis. How do you approach the analysis. The answer is - it depends on the flavor of analysis. 

Lets suppose first that you are using a standard linear model. In this case, the gold standard is to use weighted regression. This is not a regression class, so we won't get too deep into the weeds with this, but lets review breifly how a least squares regression analysis works. The goal with any of these models is that you give it some basic model structure - some linear (additive) combination of predictor variables, and you want to get back a coefficient that reflects how a change in the value of each predictor is related to change in the response variable (all other predictors being equal). These values are calculated, underthe hood, to minimize the sum of squared errors - which is just jargon for the sum of the residuals squared. You can visualize this process as the sum of "boxes" formed between the expected value of the model and each observed data point. 

![Least Squares Model](LSOpt.jpeg)



In any normal analysis, each datapoint is given equal weight. Now, because we are using squared error, the farther a datapoint falls from the model, the larger square will grow (quadratically), and the more leverage it will have on the model. But each observation here has the same opportunity for leverage - no datapoint is given any more or less weight intrinsically. 

With weighted regression, we tell the model to give certain datapoints more weight in this least squares calculation. You can visualize this as still minimizing the sum of the boxes, but different datapoints will have boxes with different densities. This gives some data point more intrinsic influence on a model than others. This creates an opportunity the bias the math in our model to counter-act the bias in our sample.

To show how this works, lets concoct a ridiculous example (loosely based on the really interesting book "Invisible Women: Data Bias in a World Designed for Men"). Suppose you work as a data scientist for LinkedIn, and they want to identify factors that influence change in their users incomes between this and the previous year. Lets suppose you have a list to mostly useless factors, like "Did you sign up for a LinkedIn learning course" and "Did you custom order a boogie zoom background" or "Did you start drinking that nasty looking mushroom coffee" that they have a bagillion adds about. But then at the very end someone from the DEI team ask you to add "have you had a baby in the last year" as a control question.

You create an anonymized survey and send it out to some platform users. Lets suppose you have 150 respondents that haven't recently had a kiddo, and the other survey variable are nonsense, so their changes in income vary randomly between a -100% and +100% change in income. But another 150 respondents have had a kid. Previous research has shown that its more common for women to have to cut back on work to care for a child, and thus depress their annual income, while men tend to work additional hours to help cover medical expenses and increase their hours (there's also some really interesting research that men with children are more promotable but women less). Lets assume the same pattern exists in your dataset. We'll suppose that 100 of the responses come from men, who saw a 0-100% increase in income, but then the women were too busy with the kids to answer your survey, and you only got 50 responses from them, all of whome saw a 0-100% reduction in income. Lets simulate some data to mimic this scenario

```{r}

set.seed(6191)

others <- runif(150, -1,1)
dads <- runif(100, 0,1)
moms <- runif(50, -1, 0)

mydata <- data.frame(Gender = c(sample(c('Men', 'Women'),150, replace = T),
                                rep('Men', 100), 
                                rep('Women', 50)),
                     ChangeIncome = c(others, dads, moms),
                     HadBaby = c(rep(F,150),rep(T,150)), 
                     LinkCourse = sample(c(T,F), 300, replace = T),
                     ZoomBackground = sample(c(T,F), 300, replace = T),
                     MushroomCoffee = sample(c(T,F), 300, replace = T)
                     )

qplot(mydata$ChangeIncome, xlab = 'Percent Change Income', bins = 15)

```

So, hopefully, in any EDA, if you saw bimodality like that, you'd take a step back and figure out what is lurking in your dataset before runing any analyses. But lets suppose your on a tight deadline, and you move ahead with your linear model

```{r}

lmout <- lm(ChangeIncome ~ HadBaby + LinkCourse + ZoomBackground + MushroomCoffee,
            data = mydata)
summary(lmout)

```
So now you go to your boss and report that none of the online learning courses or terrible fungus coffee or premium zoom subscription had any appreciable effect on income. But there was a significant positive effect for having a baby. So clearly if your users want to make more money they need to start having more children. 

Clearly that interpretation isn't going to go very far. And your probably going to end up with another email from your DEI team to double-check your demographic questions. And you of course find that you have half as many moms in your sample as you should. So lets re-run this analysis where we give the moms that did respond twice as much weight in the lm model

```{r}

mydata$weights <- c( rep(1,250), rep(2,50))

wlmout <- lm(ChangeIncome ~ HadBaby + LinkCourse + ZoomBackground + MushroomCoffee,
            data = mydata, weights = mydata$weights)
summary(wlmout)

```
And now that we've corrected the bias, we are no longer recommending having babies as a career-boosting strategy. 


Nearly any algorithm with a least-squares engine under the hood will have a weights option to add to the model optimization. Weighting can often is also be automatically implemented in a number of clustering algorithms. With any unsupervised machine learning algorithm, which has a dissimilarity matrix under the hood, distance metrics can often be modified/reweighted in a similar manner. It is my recomendation to always try to find a way to implement reweighting within the model itself, but occasionally that isn't feasible with off-the shelf algorithms. So lets also cover what to do when model reweighting isn't an option. 

# Dealing with Sampling Bias - Upsampling

Upsampling is designed to mimick model reweighting in correcting for bias - but in this approach the data input is modified, not the model itself. Its basic approach is pretty simple - if you know a subset of your data is undersampled, then you resample (clone) observational units from this subpopulation until the size of the subsample of the data mirrors that demographic in the reference population. In my experience, this usually isn't implemented as a true bootstrap - a single point is never resampled more than once. 

In theory, doubling up points should mimic the effect of reweighting - instead of giving a point double weight within the algorithm, just have two points cloned pulling density. As I warned last week, however, you need to be careful managing the bias-variance tradeoff with upsampled data. Yes a cloned observation will enhance the "pull" of the original datapoint, but it will also make the variability in your dataset look artificially low. If you are doing a large amount of upsampling, you may want to think about adding some artificial white noise into the synthetic datapoint to help smooth-out the augmented dataset. 


# Limitations

Even with re-weighting and upsampling, inferences gleaned from datasets with known sampling bias should always be taken with a grain of salt. Even if you rebalance a dataset using demographics, there may still be loss of information that you can't account for. Did you get enough samples of a subpopulation to effectively characterize their full range of values and characterize their variance? You can't create information that isn't there. Even more challenging, if there is some mechanisms causing a subpopulation to be down-sampled, can you be sure the subpopulation that you did capture is representative of the larger subpopulation, or are they themselves skewed by the sampling scheme. Such a question can seldom be answered from the data itself, and certainly can't be corrected - in such a scenario reweighting might in fact only serve to enhance the sampling bias. Creating synthetic data using an assumed model and parametric simulation techniques could help to rebalance, but doing so you typically only enforce a model you expect to find. 

I wish I could give you more complete answers to these analytically questions. But sampling bias is one of the most fundamental challenges we face as data scientists working with real world and often messy data. To my knowledge, no one has found a way around the mantra "Crap in, Crap out", and if anyone ever tells you that they have you should be incredibly wary. Such data challenges can lead to some really deep conversations with domain experts that can pay dividends in future projects. And remember - no answer is always better than a wrong inference, even if a non-response may feel like a disappointment to some consultants, it will also earn the respect and trust of others. 

# HW

Alright guys, here's a fun light assignment heading into your thanksgiving break (and to get you in the headspace for your final project). 

Explain to me, in terms of sampling bias, what is happening in this meme. How might this issue might have been avoided to allow for a successful overthrow by the machines?

![HW](SamplBiasMeme.jpeg) 




