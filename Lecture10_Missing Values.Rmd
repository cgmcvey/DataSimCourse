---
title: 'Lecture 10: Missing Values'
author: "Catie McVey"
date: "2023-10-3"
output: html_document
---

```{r}
library(tidyverse)
```


Alright, now that we've covered all the core tools in both the nonparametric and parametric toolkits, we're going to start shifting into case studies to see how these approaches stand up to problems in real world data. 

Today we're going to talk about one of the most common headaches encountered with real world data. Even in highly controlled experiments, you'll often find gaps in the records. In fact, missing values are so common, it should raise red flags if you are ever given a dataset in a consulting setting where there are no gaps in the data. Many sensor systems automatically dump incomplete or corrupted values. And in some cases, your domain expert that generated the data may have taken them out before they come to you, thinking that they will save $ on consulting hours if they do that datascrubbing up front. You can't of course fix what you can't see. So if your data is coming from another source, its always a good idea of getting in the habit of asking them what if any data scrubbing has been done prior to it landing in your lap. And if you are working with sensor records, where your domain expert might not know what all is going on underthe hood, its good to take 15 minutes to do some data visualization of the distribution of the time/space indexing.

So, lets assume you'v correctly identified gaps in your dataset. We'll start out by covering different strategies for HOW to go about filling these holes, cover their pros and cons, and then we'll end with a discussion of SHOULD you fill holes in your data.

# Imputation

To explore imputation strategies, lets set up a case study. Suppose we want to analyze data for a controlled feed trials of baby pigs. For simplicity, we'll assume that these piglets grow consistently (linearly), and that rate of gain varies uniformly across this population between 1-2 lbs/day. Lets generate a dataset of 100 piggos that we'll be tracking daily over a 50-day period. They'll all start out between 30-40 lbs. Weights of course can't be measured precisely, and there are natural day-to-day fluctuations in weight gain, so we'll assume a varience from the trajected growth curve is ~Normal(0,2)

```{r}

set.seed(61916)

mypigs <- array(NA, 
                dim = c(100,50), 
                dimnames = list(paste('Pig',1:100,sep=''), 1:50))

mygains <- rep(NA,100)
mystart <- rep(NA,100)
for(i in 1:nrow(mypigs)){
  
  mygains[i] <- runif(1, 1, 2)
  mystart[i] <- runif(1, 30, 40)
  mypigs[i,] <- (mygains[i] * 1:50 + mystart[i]) + rnorm(50,0,2) 
  
}

qplot(1:50, mypigs[1,], main = 'Example Pig')

temp <- as.data.frame(mypigs)
temp$Pig <- rownames(temp)
mypigs_long <- gather(temp,'Day', 'Weight', 1:50)
mypigs_long$Day <- as.numeric(mypigs_long$Day)

qplot(mypigs_long$Day, mypigs_long$Weight, main = 'All Pigs')

```


So if we managed to record all these weight records from all these pigs successfully, we'd fit the following model for weight vs days 

```{r}

mypigs_long_ref <- mypigs_long
mylmfit_ref <- lm(Weight ~ Day, data = mypigs_long_ref)
summary(mylmfit_ref)
mean(mylmfit_ref$residuals^2) # MSE

```


Alright, now lets suppose this data set was generated by running our pigs over a weigh scale using an RFID ear chip. Most days this works well. But lets suppose that 5% of the time, the RFID fails to read, ad the weight isn't recorded. We'll suppose for now that this occurs entirely at random. 


```{r}

set.seed(61916)

pmiss <- 0.05
mypigs_long$Weight[sample(1:nrow(mypigs_long), 
                          round(pmiss*nrow(mypigs_long)),
                          replace = F
                          )] <- NA


```


So how might we go about filling in these missing values. A standard option would be to fill in with a mean. Typically this is the mean for a given variable, so for this time series example, that would be the mean for each day of growth. That would then be equivalent to replacing each missing value with the corresponding point on the fitted line. Can you think of any problem this might cause?

```{r}

mypigs_long_meanimp <- mypigs_long
mypigs_long_meanimp$Weight[is.na(mypigs_long$Weight)] <- mylmfit_ref$fitted.values[is.na(mypigs_long$Weight)]

qplot(mypigs_long_meanimp$Day, mypigs_long_meanimp$Weight)

mylmfit_meanimp <- lm(Weight ~ Day, data = mypigs_long_meanimp)
summary(mylmfit_meanimp)
mean(mylmfit_meanimp$residuals^2) # MSE

```


As you can see, by simply replacing our missing values with the mean/expected values, we've artificially deflated our Mean Squared Error (MSE) estimate. Now, in linear models, MSE is used in most of our anova/coefficient tests, so making it artificially smaller will alter our p-values. And making our model look more certain than it actually is will usually increase the risk of a false-positive result. 

So can we think of any better options? 

.....



Well, we could look at this parametrically, right. Filling in with our expected value is a good starting point, but too informed, so why not re-infuse some stochasticity back into the imputations. To do this we'd simply need to make an assumption about how the residuals are distributed - in this case we're assuming normal for the linear model tests, so might as well assume it in the imputation. Then we just need a reasonable estimate of the standard deviation, right, and then we can stochasitcally sample residual error to add back into the imputation. 

But what is a good variance estimation here? Growth curves are highly studied in pigs, so we might use a book value and hide behind a citation. Or, if the data isn't too gappy around our missing datapoint, then we can use an empirical estiamte from the data itself. In this case you should notice that variance isn't uniform across this domain, the bigger the pigs get, their growth curves diverge, the greater the variance, so we need to condition the sampling of the residual error term by day of observation.


```{r}

set.seed(61916)
mypigs_long_paramimp <- mypigs_long

for(i in 1:nrow(mypigs_long)){
  if(is.na(mypigs_long$Weight[i])){
    
    tempsd <- sd(subset(mypigs_long$Weight, mypigs_long$Day == mypigs_long$Day[i]),
                 na.rm = T)
    mypigs_long_paramimp$Weight[i] <- mylmfit_ref$fitted.values[i] + rnorm(1, 0, tempsd)
    
  }
}


qplot(mypigs_long_paramimp$Day, mypigs_long_paramimp$Weight)

mylmfit_paramimp <- lm(Weight ~ Day, data = mypigs_long_paramimp)
summary(mylmfit_paramimp)
mean(mylmfit_paramimp$residuals^2) # MSE

```

That MSE is now extremely close to the original value, which should help us avoid risk of influencing our p-values. 

So why don't you hear this approach more commonly. Well, most folks that don't like stochastic imputation for the same reason they don't like nonparametric inferences - because they don't get exactly the same result every time. Of course my arguement is, as always, that with a stochasic system nothing is certain - that the closed formed equations that give you the same p-values each time with standard linear models are an artificial certainty. And to be fair, if infusing a reasonable amount of noise into missing value estimates is posing a serious risk to the stability of your inference, then you probably have other deeper problems in your dataset that need to be addressed. 

But lets say that we aren't comfortable making parametric assumptions about our dataset - maybe we are worried the gaps are too large in the data for empirical estimates, and don't have enough background on the study system to make assumptions. Can you think of a nonparametric way to approach this issue?

.....

We can always use our existing sample as a proxy for our reference distribution and draw from it. The trick, however, is to make sure we're not comparing apples and oranges. In this example, if we sampled from completely at random from the weight records, would we get a reasonable substitute? Maybe but maybe not - if we happened to sample from a distant time point relative to that of the missing value, we might get a weight value that would act as an outlier. To avoid this we might simply sample only from observations with the same time point.


```{r}

set.seed(61916)
mypigs_long_nparimp <- mypigs_long

for(i in 1:nrow(mypigs_long)){
  if(is.na(mypigs_long$Weight[i])){
    
    tempsamp <- subset(mypigs_long$Weight, mypigs_long$Day == mypigs_long$Day[i])
    mypigs_long_nparimp$Weight[i] <- sample(tempsamp,1)
    
  }
}


qplot(mypigs_long_nparimp$Day, mypigs_long_nparimp$Weight)

mylmfit_nparimp <- lm(Weight ~ Day, data = mypigs_long_nparimp)
summary(mylmfit_nparimp)
mean(mylmfit_nparimp$residuals^2) # MSE

```

Righto, even closer to our reference MSE. 

So what is the potential downside? Well, again, if your missing value falls in an under-sampled section of your reference distribution, you may not have many options there to sample from and fill in with. This can be particularly hard to assess if you are working with high dimensional data. What is "similar" enough.

One related strategy designed to deal with this issue is nearest-neighbor replacement. Here you take your observation with one field missing, and compare it with all other observation across the remaining complete axes - in other works its k = 1 nearest neighbor. You then fill in the missing field with the complete feild of the most similar observation. 

This approach tends to work well so long as missing values are reasonably evenly distributed in a dataset. If, however, your missing value tend to cluster together, and there aren't many other complete points nearby, you can inadvertently replicate the same observation many times to fill in the missing value. This can artificially deflate the variance in a linear model. Additionally, if you are using a machine learning model, you should know that filling in a lot of point on top of each other will distort the density field, creating a "density sink" for the tuning model to want to get stuck in, making it hard to control the bias-variance tradeoff in that region. 


# When to Impute

Alright, now that we know some better ways to fill in missing data, we should now talk about why. 

In the example above, our missing values were driven by the sensor itself, and so randomly distributed throughout the dataset. So, if we'd just ignored the holes, we'd have likely been alright in our analyses. 

With some modern sensor technologies the rates of missing values can be quite high, even exceeding 10%. In such cases, even if missing values are dispersed entirely at random, we could loose a significant number of complete observations, hemorhaging power from our statistical analyses. In such cases imputing missing values can allow us to recover a significant amount of information. 

But what if missing values aren't random. Like we saw in our previous example with dying cows in the feedlot when fed a treatment ration. If there is an underlying mechanism driving records to be dropped from a dataset, this can lead to sampling bias, which can obscure or even invert statistical inferences. In these cases, imputation offers an opportunity to re-balance our dataset, assuming we can set up an imputation stratedgy that counters the mechanism driving the bias. 

How do we know whether missing values are random or biased? In many cases, you'll need to fall back on your domain expert to answer this question, or even encourage them to go back and question whoever directly collected data. But that doesn't releive you of your duties as a statistician to try to identify biases empirically. Here EDA will be your best friend. Can you think of a technique from out EDA lecture that might be particularly well suited to this task?

For multivariate datasets, heatmap visualizations where observations are clustered via unspuervised machine learning algorithms can be invaluable to identifying signs of biased data loss. These algorithms will cluster observations with similar values across all data fields (time indices, spatial indices, predictive variables, etc). If missing values occur entirely at random, blank cells will be scattered evenly throughout the heatmap. If there is some underlying variable driving missing records, then when observations are clustered using complete variables, then empty cells will also cluster together on the heatmap visualization. 


# HW

Go back to our example from the nonparametric module where cows died during our feed trial. If they died at random at equivalent rates on either died, should we impute the missing values? If yes, how? 

What if the rates of disease were higher on the treatment diet. Would imputation have helped or hurt us on our question for a reliable statistical inference? If you think we should have imputed, how should we have done so?

What if, independent of diet, the faster a cow grows the more likely they are to die (putting more energy into growth than imunity). Should we impute in this circumstance? If yes how would you propose we do so?













